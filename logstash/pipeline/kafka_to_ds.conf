input {
  kafka {
    bootstrap_servers => "${KAFKA_BOOTSTRAP_SERVERS}"
    topics => ["${KAFKA_TOPICS}"]
    codec => "json"
    group_id => "logstash_consumer"
    auto_offset_reset => "earliest"
    consumer_threads => 3
  }
}

filter {
  # Add timestamp from the message
  date {
    match => ["timestamp", "UNIX"]
    target => "@timestamp"
  }
  
  # Save 'name' to 'ds_name' metadata to avoid conflicts with 'name' tag for requests
  mutate {
    add_field    => { "[@metadata][ds_name]" => "%{name}" }
    remove_field => [ "name" ]
  }
  
  # Flatten every key/value under "tags" to top-level fields
  ruby {
    code => "
      # Handle tags field if it exists
      tags = event.get('tags')
      if tags
        if tags.is_a?(Hash)
          # If tags is a hash, process key-value pairs
          tags.each { |k, v| event.set(k, v) }
        elsif tags.is_a?(Array) && tags.include?('_tagsparsefailure')
          # If tags indicates a parse failure, check for _tags
          _tags = event.get('_tags')
          if _tags && _tags.is_a?(Array)
            # Process each tag object in the array
            _tags.each do |tag_obj|
              if tag_obj.is_a?(Hash)
                tag_obj.each { |k, v| event.set(k, v) }
              end
            end
          end
        end
      end
    "
  }

  # Flatten every key/value under "fields" to top-level fields
  ruby {
    code => "
      fields = event.get('fields')
      if fields && fields.is_a?(Hash)
        fields.each { |k, v| event.set(k, v) }
      end
    "
  }

  # Add relativeTimestamp from the relativeTime field
  date {
    match => ["relativeTime", "UNIX_MS"]
    target => "@relativeTimestamp"
  }

  # Convert runId to integer for filtration and indexing
  mutate {
    convert => {"runId" => "integer"}
  }

  # Remove processed fields to clean up the event
  mutate {
    remove_field => [ "tags",
                      "_tags",
                      "fields",
                      "event",
                      "influxdb_backet",
                      "timestamp",
                      "relativeTime",
                      "host"              # Remove telegraf host field
                    ]
  }
}

output {
  opensearch {
    hosts => ["${OPENSEARCH_HOSTS}"]
    index => "${OPENSEARCH_DS_PATTERN}"
    action => "create"
    user => "${OPENSEARCH_USER}"
    password => "${OPENSEARCH_PASSWORD}"
    ssl => true
    ssl_certificate_verification => false
  }
  
  # Uncomment log to stdout for debugging
  # stdout {
  #   codec => rubydebug
  # }

  # Use name field to dynamically set file path and index name
  # Uncomment the following lines to enable file output for debugging
  # file {
  #   path => "/usr/share/logstash/output/%{[@metadata][ds_name]}.json"
  #   codec => json_lines
  # }
}